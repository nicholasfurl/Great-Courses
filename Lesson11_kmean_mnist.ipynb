{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtSS+gqT/XWTu3THNfT1HP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicholasfurl/Great-Courses/blob/main/Lesson11_kmean_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8GqbT6qp_h9"
      },
      "outputs": [],
      "source": [
        " #we will write our own k-means model from scratch and use it to cluster handwritten numbers from the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unlike the lesson, I like to start with the main code body and then explore the \n",
        "#function definitions when I know what data they are being passed\n",
        "\n",
        "#Data is a \"bunch\"\n",
        "#Lesson ten passed fetch_openmil an argument return_X_y=True and it returned two pandas arrays instead\n",
        "#What is a bunch?\n",
        "#Bunch acts like an object and a dict.\n",
        "#>>> b = Bunch()\n",
        "#>>> b.hello = 'world'\n",
        "#>>> b.hello\n",
        "#'world'\n",
        "#>>> b['hello'] += \"!\"\n",
        "#>>> b.hello\n",
        "from sklearn.datasets import fetch_openml\n",
        "data = fetch_openml(name='mnist_784')\n"
      ],
      "metadata": {
        "id": "VLHKIE7Pu61E",
        "outputId": "95c36a27-b96c-44df-d155-b7c5eb1f0943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The fetch command takes a long time so I've put it in a cell by itself.\n",
        "#33% of data in data allocated to test data\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.33)\n",
        "len(X_train)"
      ],
      "metadata": {
        "id": "Q9lFPy32w7ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_data(data,centers):\n",
        "  \n",
        "  # n is the number of data points\n",
        "  n = len(data)\n",
        "  # d is the dimensionality of the data points\n",
        "  d = len(data[0])\n",
        "  # k is the number of clusters\n",
        "  k = len(centers)\n",
        "  # first, subtract the set of centers from each data point\n",
        "  res = np.reshape(data,(1,n,d))-np.reshape(centers,(k,1,d))\n",
        "  # sum the squared differences\n",
        "  res2 = np.add.reduce(res**2,2)\n",
        "  # assign each data point to its closest center\n",
        "  centerids = np.apply_along_axis(np.argmin,0,res2)\n",
        "  # While we're here, make a note of the loss\n",
        "  loss = sum(np.apply_along_axis(np.min,0,res2))\n",
        "  return(centerids, loss)"
      ],
      "metadata": {
        "id": "b2jMGWw4t2dh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}